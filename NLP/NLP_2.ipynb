{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docx.Document(r'C:\\E\\E_drive_copy\\sample_data\\nlp\\ek\\test_doc.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = doc.paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brexit is the impending  of the  (UK) from the  (EU). In a , a majority of British voters supported leaving the EU. On 29 March 2017, the  invoked . The  declares \"exit day\" to be 29 March 2019 at 11 p.m. ().\n"
     ]
    }
   ],
   "source": [
    "para = doc.paragraphs[0].text\n",
    "\n",
    "print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brexit', 'is', 'the', 'impending', 'of', 'the', '(', 'UK', ')', 'from', 'the', '(', 'EU', ')', '.', 'In', 'a', ',', 'a', 'majority', 'of', 'British', 'voters', 'supported', 'leaving', 'the', 'EU', '.', 'On', '29', 'March', '2017', ',', 'the', 'invoked', '.', 'The', 'declares', '``', 'exit', 'day', \"''\", 'to', 'be', '29', 'March', '2019', 'at', '11', 'p.m.', '(', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brexit\\xa0is the impending\\xa0\\xa0of the\\xa0\\xa0(UK) from the\\xa0\\xa0(EU).', 'In a\\xa0, a majority of British voters supported leaving the EU.', 'On 29 March 2017, the\\xa0 invoked\\xa0.', 'The\\xa0\\xa0declares \"exit day\" to be 29 March 2019 at 11\\xa0p.m.\\xa0().']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Brexit': 1, 'is': 1, 'the': 5, 'impending': 1, 'of': 2, '(': 3, 'UK': 1, ')': 3, 'from': 1, 'EU': 2, '.': 4, 'In': 1, 'a': 2, ',': 2, 'majority': 1, 'British': 1, 'voters': 1, 'supported': 1, 'leaving': 1, 'On': 1, '29': 2, 'March': 2, '2017': 1, 'invoked': 1, 'The': 1, 'declares': 1, '``': 1, 'exit': 1, 'day': 1, \"''\": 1, 'to': 1, 'be': 1, '2019': 1, 'at': 1, '11': 1, 'p.m.': 1}\n"
     ]
    }
   ],
   "source": [
    "word_count = FreqDist(words)\n",
    "\n",
    "print(dict(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 5, '.': 4, '(': 3, ')': 3, 'of': 2, 'EU': 2, 'a': 2, ',': 2, '29': 2, 'March': 2, ...})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brexit', 'is'),\n",
       " ('is', 'the'),\n",
       " ('the', 'impending'),\n",
       " ('impending', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', '('),\n",
       " ('(', 'UK'),\n",
       " ('UK', ')'),\n",
       " (')', 'from'),\n",
       " ('from', 'the'),\n",
       " ('the', '('),\n",
       " ('(', 'EU'),\n",
       " ('EU', ')'),\n",
       " (')', '.'),\n",
       " ('.', 'In'),\n",
       " ('In', 'a'),\n",
       " ('a', ','),\n",
       " (',', 'a'),\n",
       " ('a', 'majority'),\n",
       " ('majority', 'of'),\n",
       " ('of', 'British'),\n",
       " ('British', 'voters'),\n",
       " ('voters', 'supported'),\n",
       " ('supported', 'leaving'),\n",
       " ('leaving', 'the'),\n",
       " ('the', 'EU'),\n",
       " ('EU', '.'),\n",
       " ('.', 'On'),\n",
       " ('On', '29'),\n",
       " ('29', 'March'),\n",
       " ('March', '2017'),\n",
       " ('2017', ','),\n",
       " (',', 'the'),\n",
       " ('the', 'invoked'),\n",
       " ('invoked', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'declares'),\n",
       " ('declares', '``'),\n",
       " ('``', 'exit'),\n",
       " ('exit', 'day'),\n",
       " ('day', \"''\"),\n",
       " (\"''\", 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', '29'),\n",
       " ('29', 'March'),\n",
       " ('March', '2019'),\n",
       " ('2019', 'at'),\n",
       " ('at', '11'),\n",
       " ('11', 'p.m.'),\n",
       " ('p.m.', '('),\n",
       " ('(', ')'),\n",
       " (')', '.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "bigram_text = list(nltk.bigrams(words))\n",
    "\n",
    "bigram_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brexit', 'is', 'the'),\n",
       " ('is', 'the', 'impending'),\n",
       " ('the', 'impending', 'of'),\n",
       " ('impending', 'of', 'the'),\n",
       " ('of', 'the', '('),\n",
       " ('the', '(', 'UK'),\n",
       " ('(', 'UK', ')'),\n",
       " ('UK', ')', 'from'),\n",
       " (')', 'from', 'the'),\n",
       " ('from', 'the', '('),\n",
       " ('the', '(', 'EU'),\n",
       " ('(', 'EU', ')'),\n",
       " ('EU', ')', '.'),\n",
       " (')', '.', 'In'),\n",
       " ('.', 'In', 'a'),\n",
       " ('In', 'a', ','),\n",
       " ('a', ',', 'a'),\n",
       " (',', 'a', 'majority'),\n",
       " ('a', 'majority', 'of'),\n",
       " ('majority', 'of', 'British'),\n",
       " ('of', 'British', 'voters'),\n",
       " ('British', 'voters', 'supported'),\n",
       " ('voters', 'supported', 'leaving'),\n",
       " ('supported', 'leaving', 'the'),\n",
       " ('leaving', 'the', 'EU'),\n",
       " ('the', 'EU', '.'),\n",
       " ('EU', '.', 'On'),\n",
       " ('.', 'On', '29'),\n",
       " ('On', '29', 'March'),\n",
       " ('29', 'March', '2017'),\n",
       " ('March', '2017', ','),\n",
       " ('2017', ',', 'the'),\n",
       " (',', 'the', 'invoked'),\n",
       " ('the', 'invoked', '.'),\n",
       " ('invoked', '.', 'The'),\n",
       " ('.', 'The', 'declares'),\n",
       " ('The', 'declares', '``'),\n",
       " ('declares', '``', 'exit'),\n",
       " ('``', 'exit', 'day'),\n",
       " ('exit', 'day', \"''\"),\n",
       " ('day', \"''\", 'to'),\n",
       " (\"''\", 'to', 'be'),\n",
       " ('to', 'be', '29'),\n",
       " ('be', '29', 'March'),\n",
       " ('29', 'March', '2019'),\n",
       " ('March', '2019', 'at'),\n",
       " ('2019', 'at', '11'),\n",
       " ('at', '11', 'p.m.'),\n",
       " ('11', 'p.m.', '('),\n",
       " ('p.m.', '(', ')'),\n",
       " ('(', ')', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_text = list(nltk.trigrams(words))\n",
    "\n",
    "trigram_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brexit', 'is', 'the', 'impending', 'of'),\n",
       " ('is', 'the', 'impending', 'of', 'the'),\n",
       " ('the', 'impending', 'of', 'the', '('),\n",
       " ('impending', 'of', 'the', '(', 'UK'),\n",
       " ('of', 'the', '(', 'UK', ')'),\n",
       " ('the', '(', 'UK', ')', 'from'),\n",
       " ('(', 'UK', ')', 'from', 'the'),\n",
       " ('UK', ')', 'from', 'the', '('),\n",
       " (')', 'from', 'the', '(', 'EU'),\n",
       " ('from', 'the', '(', 'EU', ')'),\n",
       " ('the', '(', 'EU', ')', '.'),\n",
       " ('(', 'EU', ')', '.', 'In'),\n",
       " ('EU', ')', '.', 'In', 'a'),\n",
       " (')', '.', 'In', 'a', ','),\n",
       " ('.', 'In', 'a', ',', 'a'),\n",
       " ('In', 'a', ',', 'a', 'majority'),\n",
       " ('a', ',', 'a', 'majority', 'of'),\n",
       " (',', 'a', 'majority', 'of', 'British'),\n",
       " ('a', 'majority', 'of', 'British', 'voters'),\n",
       " ('majority', 'of', 'British', 'voters', 'supported'),\n",
       " ('of', 'British', 'voters', 'supported', 'leaving'),\n",
       " ('British', 'voters', 'supported', 'leaving', 'the'),\n",
       " ('voters', 'supported', 'leaving', 'the', 'EU'),\n",
       " ('supported', 'leaving', 'the', 'EU', '.'),\n",
       " ('leaving', 'the', 'EU', '.', 'On'),\n",
       " ('the', 'EU', '.', 'On', '29'),\n",
       " ('EU', '.', 'On', '29', 'March'),\n",
       " ('.', 'On', '29', 'March', '2017'),\n",
       " ('On', '29', 'March', '2017', ','),\n",
       " ('29', 'March', '2017', ',', 'the'),\n",
       " ('March', '2017', ',', 'the', 'invoked'),\n",
       " ('2017', ',', 'the', 'invoked', '.'),\n",
       " (',', 'the', 'invoked', '.', 'The'),\n",
       " ('the', 'invoked', '.', 'The', 'declares'),\n",
       " ('invoked', '.', 'The', 'declares', '``'),\n",
       " ('.', 'The', 'declares', '``', 'exit'),\n",
       " ('The', 'declares', '``', 'exit', 'day'),\n",
       " ('declares', '``', 'exit', 'day', \"''\"),\n",
       " ('``', 'exit', 'day', \"''\", 'to'),\n",
       " ('exit', 'day', \"''\", 'to', 'be'),\n",
       " ('day', \"''\", 'to', 'be', '29'),\n",
       " (\"''\", 'to', 'be', '29', 'March'),\n",
       " ('to', 'be', '29', 'March', '2019'),\n",
       " ('be', '29', 'March', '2019', 'at'),\n",
       " ('29', 'March', '2019', 'at', '11'),\n",
       " ('March', '2019', 'at', '11', 'p.m.'),\n",
       " ('2019', 'at', '11', 'p.m.', '('),\n",
       " ('at', '11', 'p.m.', '(', ')'),\n",
       " ('11', 'p.m.', '(', ')', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_text = list(nltk.ngrams(words, 5))\n",
    "\n",
    "ngram_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming (process of removing duplicate words. example- run, running will be 2 words in tokens.)\n",
    "# the same will be rediced to 1 after stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) porter stemmer (using porters algorithm)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run  :  run\n",
      "runs  :  run\n",
      "ran  :  ran\n",
      "running  :  run\n",
      "run  :  run\n",
      "runner  :  runner\n",
      "easily  :  easili\n",
      "fairly  :  fairli\n"
     ]
    }
   ],
   "source": [
    "word_list = ['run', 'runs', 'ran', 'running', 'run', 'runner','easily', 'fairly']\n",
    "\n",
    "for i in word_list:\n",
    "    print(i, ' : ', ps.stem(i))\n",
    "    \n",
    "# PorterStemmer cannot find all stems successfully. it looks ok to an extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run  :  run\n",
      "runs  :  run\n",
      "ran  :  ran\n",
      "running  :  run\n",
      "run  :  run\n",
      "runner  :  runner\n",
      "easily  :  easili\n",
      "fairly  :  fair\n"
     ]
    }
   ],
   "source": [
    "# snowball stemmer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "sbs = SnowballStemmer(language='english')\n",
    "\n",
    "for i in word_list:\n",
    "    print(i, ' : ', sbs.stem(i))\n",
    "    \n",
    "# performance is better than porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbs.languages # languages suppots by snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run  :  run\n",
      "runs  :  run\n",
      "ran  :  ran\n",
      "running  :  run\n",
      "run  :  run\n",
      "runner  :  run\n",
      "easily  :  easy\n",
      "fairly  :  fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "for i in word_list:\n",
    "    print(i, ' : ', ls.stem(i))\n",
    "    \n",
    "# result looks better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'planning', 'to', 'meet', 'him', 'at', 'the', 'meeting', 'if', 'he', 'is', 'coming', 'for', 'the', 'meeting']\n"
     ]
    }
   ],
   "source": [
    "str1 = 'i am planning to meet him at the meeting if he is coming for the meeting'\n",
    "\n",
    "str1_wds = word_tokenize(str1)\n",
    "\n",
    "print(str1_wds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(wl):\n",
    "    porter, snow, lan = [], [], []\n",
    "    for i in wl:\n",
    "        porter.append(ps.stem(i))\n",
    "    for i in wl:\n",
    "        snow.append(sbs.stem(i))\n",
    "    for i in wl:\n",
    "        lan.append(ls.stem(i))\n",
    "    return(porter, snow, lan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['i', 'am', 'plan', 'to', 'meet', 'him', 'at', 'the', 'meet', 'if', 'he', 'is', 'come', 'for', 'the', 'meet'], ['i', 'am', 'plan', 'to', 'meet', 'him', 'at', 'the', 'meet', 'if', 'he', 'is', 'come', 'for', 'the', 'meet'], ['i', 'am', 'plan', 'to', 'meet', 'him', 'at', 'the', 'meet', 'if', 'he', 'is', 'com', 'for', 'the', 'meet'])\n"
     ]
    }
   ],
   "source": [
    "print(stemmer(str1_wds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'plan', 'to', 'meet', 'him', 'at', 'the', 'meet', 'if', 'he', 'is', 'come', 'for', 'the', 'meet']\n",
      "['i', 'am', 'plan', 'to', 'meet', 'him', 'at', 'the', 'meet', 'if', 'he', 'is', 'come', 'for', 'the', 'meet']\n",
      "['i', 'am', 'plan', 'to', 'meet', 'him', 'at', 'the', 'meet', 'if', 'he', 'is', 'com', 'for', 'the', 'meet']\n"
     ]
    }
   ],
   "source": [
    "print(stemmer(str1_wds)[0])\n",
    "print(stemmer(str1_wds)[1])\n",
    "print(stemmer(str1_wds)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sreer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# stemming actually reduces the number of words. lemmetization trying to find root of the word also\n",
    "\n",
    "from nltk.stem import wordnet\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'runs', 'ran', 'running', 'run', 'runner', 'easily', 'fairly']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run  :  run\n",
      "runs  :  run\n",
      "ran  :  ran\n",
      "running  :  running\n",
      "run  :  run\n",
      "runner  :  runner\n",
      "easily  :  easily\n",
      "fairly  :  fairly\n"
     ]
    }
   ],
   "source": [
    "for i in word_list:\n",
    "    print(i, ' : ', wnl.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sreer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "st_words_eng = stopwords.words('english')\n",
    "\n",
    "print(st_words_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(st_words_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2 = 'python is awsome. it is easy to learn. its too easy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'is', 'awsome', '.', 'it', 'is', 'easy', 'to', 'learn', '.', 'its', 'too', 'easy']\n"
     ]
    }
   ],
   "source": [
    "str2_wrd = word_tokenize(str2)\n",
    "\n",
    "print(str2_wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', 'awsome', '.', 'easy', 'learn', '.', 'easy']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stopwords\n",
    "\n",
    "[i for i in str2_wrd if i not in st_words_eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'is', 'awsome', 'it', 'is', 'easy', 'to', 'learn', 'its', 'too', 'easy']\n"
     ]
    }
   ],
   "source": [
    "# tokenize using regex tokenizer to remove punctuations\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "rt = RegexpTokenizer(r'\\w+')\n",
    "words_rgx_tok = rt.tokenize(str2)\n",
    "\n",
    "print(words_rgx_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', 'awsome', 'easy', 'learn', 'easy']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stopwords after applying regex tokenizer\n",
    "\n",
    "[i for i in words_rgx_tok if i not in st_words_eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "str3 = '''He watched as the young man tried to impress everyone in the room with his intelligence. There was no doubt that he was smart. The fact that he was more intelligent than anyone else in the room could have been easily deduced, but nobody was really paying any attention due to the fact that it was also obvious that the young man only cared about his intelligence.'''\n",
    "\n",
    "\n",
    "# steps in handling a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he watched as the young man tried to impress everyone in the room with his intelligence there was no doubt that he was smart the fact that he was more intelligent than anyone else in the room could have been easily deduced but nobody was really paying any attention due to the fact that it was also obvious that the young man only cared about his intelligence'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) lower the entire string and remove punctuations\n",
    "\n",
    "str3_n = ''\n",
    "for i in str3:\n",
    "    if i not in '.,()?-\\'\"':\n",
    "        str3_n += i.lower()\n",
    "        \n",
    "str3_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'watched', 'as', 'the', 'young', 'man', 'tried', 'to', 'impress', 'everyone', 'in', 'the', 'room', 'with', 'his', 'intelligence', 'there', 'was', 'no', 'doubt', 'that', 'he', 'was', 'smart', 'the', 'fact', 'that', 'he', 'was', 'more', 'intelligent', 'than', 'anyone', 'else', 'in', 'the', 'room', 'could', 'have', 'been', 'easily', 'deduced', 'but', 'nobody', 'was', 'really', 'paying', 'any', 'attention', 'due', 'to', 'the', 'fact', 'that', 'it', 'was', 'also', 'obvious', 'that', 'the', 'young', 'man', 'only', 'cared', 'about', 'his', 'intelligence']\n"
     ]
    }
   ],
   "source": [
    "# 2) tokenize the string\n",
    "\n",
    "str3_wrds = word_tokenize(str3_n)\n",
    "print(str3_wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['watched', 'young', 'man', 'tried', 'impress', 'everyone', 'room', 'intelligence', 'doubt', 'smart', 'fact', 'intelligent', 'anyone', 'else', 'room', 'could', 'easily', 'deduced', 'nobody', 'really', 'paying', 'attention', 'due', 'fact', 'also', 'obvious', 'young', 'man', 'cared', 'intelligence']\n"
     ]
    }
   ],
   "source": [
    "# 3) remove stop words\n",
    "\n",
    "str3_wrds = [i for i in str3_wrds if i not in st_words_eng]\n",
    "\n",
    "print(str3_wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'young': 2, 'man': 2, 'room': 2, 'intelligence': 2, 'fact': 2, 'watched': 1, 'tried': 1, 'impress': 1, 'everyone': 1, 'doubt': 1, ...})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) can check the frequency of clean words if required\n",
    "\n",
    "FreqDist(str3_wrds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### parts of speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sreer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS (Noun, verbs, adverbs, etc tagging)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('watched', 'VBN'), ('young', 'JJ'), ('man', 'NN'), ('tried', 'VBD'), ('impress', 'JJ'), ('everyone', 'NN'), ('room', 'NN'), ('intelligence', 'NN'), ('doubt', 'NN'), ('smart', 'JJ'), ('fact', 'NN'), ('intelligent', 'JJ'), ('anyone', 'NN'), ('else', 'JJ'), ('room', 'NN'), ('could', 'MD'), ('easily', 'RB'), ('deduced', 'VB'), ('nobody', 'NN'), ('really', 'RB'), ('paying', 'VBG'), ('attention', 'NN'), ('due', 'JJ'), ('fact', 'NN'), ('also', 'RB'), ('obvious', 'JJ'), ('young', 'JJ'), ('man', 'NN'), ('cared', 'VBD'), ('intelligence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "str3_wrds_tag = nltk.pos_tag(str3_wrds)\n",
    "\n",
    "print(str3_wrds_tag)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "               POS Taggings\n",
    "=========================================\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tThis NLTK POS Tag is an adjective (large)\n",
    "JJR\tadjective, comparative (larger)\n",
    "JJS\tadjective, superlative (largest)\n",
    "LS\tlist market\n",
    "MD\tmodal (could, will)\n",
    "NN\tnoun, singular (cat, tree)\n",
    "NNS\tnoun plural (desks)\n",
    "NNP\tproper noun, singular (sarah)\n",
    "NNPS\tproper noun, plural (indians or americans)\n",
    "PDT\tpredeterminer (all, both, half)\n",
    "POS\tpossessive ending (parent\\ ‘s)\n",
    "PRP\tpersonal pronoun (hers, herself, him, himself)\n",
    "PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
    "RB\tadverb (occasionally, swiftly)\n",
    "RBR\tadverb, comparative (greater)\n",
    "RBS\tadverb, superlative (biggest)\n",
    "RP\tparticle (about)\n",
    "TO\tinfinite marker (to)\n",
    "UH\tinterjection (goodbye)\n",
    "VB\tverb (ask)\n",
    "VBG\tverb gerund (judging)\n",
    "VBD\tverb past tense (pleaded)\n",
    "VBN\tverb past participle (reunified)\n",
    "VBP\tverb, present tense not 3rd person singular(wrap)\n",
    "VBZ\tverb, present tense with 3rd person singular (bases)\n",
    "WDT\twh-determiner (that, what)\n",
    "WP\twh- pronoun (who)\n",
    "WRB\twh- adverb (how)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER helps identify geo political entity, location, person, organization, geo socio politicalgroup, etc\n",
    "\n",
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\sreer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\sreer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  watched/VBN\n",
      "  young/JJ\n",
      "  man/NN\n",
      "  tried/VBD\n",
      "  impress/JJ\n",
      "  everyone/NN\n",
      "  room/NN\n",
      "  intelligence/NN\n",
      "  doubt/NN\n",
      "  smart/JJ\n",
      "  fact/NN\n",
      "  intelligent/JJ\n",
      "  anyone/NN\n",
      "  else/JJ\n",
      "  room/NN\n",
      "  could/MD\n",
      "  easily/RB\n",
      "  deduced/VB\n",
      "  nobody/NN\n",
      "  really/RB\n",
      "  paying/VBG\n",
      "  attention/NN\n",
      "  due/JJ\n",
      "  fact/NN\n",
      "  also/RB\n",
      "  obvious/JJ\n",
      "  young/JJ\n",
      "  man/NN\n",
      "  cared/VBD\n",
      "  intelligence/NN)\n"
     ]
    }
   ],
   "source": [
    "chu = ne_chunk(str3_wrds_tag)\n",
    "print(chu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ['India', 'USA', 'UK', 'Apple', 'Iphone', 'Samsung', 'walk', 'running']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('India', 'NNP'), ('USA', 'NNP'), ('UK', 'NNP'), ('Apple', 'NNP'), ('Iphone', 'NNP'), ('Samsung', 'NNP'), ('walk', 'VBP'), ('running', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "list1_pos_tag = nltk.pos_tag(list1)\n",
    "\n",
    "print(list1_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE India/NNP)\n",
      "  (ORGANIZATION USA/NNP)\n",
      "  UK/NNP\n",
      "  Apple/NNP\n",
      "  Iphone/NNP\n",
      "  Samsung/NNP\n",
      "  walk/VBP\n",
      "  running/VBG)\n"
     ]
    }
   ],
   "source": [
    "list1_chunk = ne_chunk(list1_pos_tag)\n",
    "print(list1_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2 = 'it was happened on 5 January 2021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S it/PRP was/VBD happened/VBN on/IN 5/CD January/NNP 2021/CD)\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(nltk.pos_tag(word_tokenize(str2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
